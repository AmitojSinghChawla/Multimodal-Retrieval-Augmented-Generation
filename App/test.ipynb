{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:05:15.014383Z",
     "start_time": "2025-11-30T10:05:14.382693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7530fc8f7e650f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:05:38.413816Z",
     "start_time": "2025-11-30T10:05:18.577182Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Multimodal-Retrieval-Augmented-Generation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from App.console_app import initialize_retriever\n",
    "from App.Ingestion_chain import ingestion_chain\n",
    "from App.retrieval_chain import answer_question, parse_docs\n",
    "from App.VectorDB import retrieve_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15374f6dbc8fdae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:05:39.035241Z",
     "start_time": "2025-11-30T10:05:38.418826Z"
    }
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    ContextPrecision,\n",
    "    AnswerRelevancy,\n",
    "    SemanticSimilarity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac3837065f7848b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:05:53.991139Z",
     "start_time": "2025-11-30T10:05:53.985709Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "load_dotenv(verbose=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675a01d64869855c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:06:04.886941Z",
     "start_time": "2025-11-30T10:06:04.882941Z"
    }
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"How does the Transformer architecture model sequence dependencies without using recurrence or convolution?\",\n",
    "    \"What are the main components of the encoder stack, and how do its sub-layers operate?\",\n",
    "    \"How does the decoder use masking to preserve the autoregressive property during generation?\",\n",
    "    \"What mathematical steps define Scaled Dot-Product Attention and why is scaling by sqrt(dk) required?\",\n",
    "    \"What benefits does Multi-Head Attention provide compared to a single attention head?\",\n",
    "    \"How do positional encodings represent token order, and why do sinusoidal functions enable extrapolation?\",\n",
    "    \"What role does the position-wise Feed-Forward Network play in each Transformer layer?\",\n",
    "    \"How does self-attention achieve a shorter maximum path length compared to RNN and CNN architectures?\",\n",
    "    \"What computational trade-offs does self-attention introduce in terms of per-layer complexity?\",\n",
    "    \"What data, batching strategy, and vocabulary were used for training on the WMT 2014 English-German task?\",\n",
    "    \"How does the learning-rate schedule with warmup affect the training dynamics of the Transformer?\",\n",
    "    \"What regularization methods (dropout, label smoothing) were applied and why?\",\n",
    "    \"What BLEU scores did the Transformer achieve on the WMT 2014 English-German translation benchmark?\",\n",
    "    \"How does the Transformer's training cost compare to earlier models like GNMT or ConvS2S?\",\n",
    "    \"What performance differences were observed when scaling from the base Transformer model to the big model?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ce8167545c84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:06:13.021912Z",
     "start_time": "2025-11-30T10:06:13.015886Z"
    }
   },
   "outputs": [],
   "source": [
    "golden_answers = [\n",
    "    \"In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\",\n",
    "    \"Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\",\n",
    "    \"We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆž) all values in the input of the softmax which correspond to illegal connections.\",\n",
    "    \"We compute the dot products of the query with all keys, divide each by âˆšdk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as: Attention(Q, K, V ) = softmax(QKT / âˆšdk)V. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by âˆš1/dk.\",\n",
    "    \"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\",\n",
    "    \"In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/10000^(2i/dmodel)) and PE(pos,2i+1) = cos(pos/10000^(2i/dmodel)), where pos is the position and i is the dimension. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos. We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\",\n",
    "    \"In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. While the linear transformations are the same across different positions, they use different parameters from layer to layer.\",\n",
    "    \"As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions, and doing so requires a stack of O(n/k) convolutional layers or O(log_k(n)) in the case of dilated convolutions, increasing the length of the longest paths between any two positions in the network.\",\n",
    "    \"One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. As noted in Table 1, the complexity per self-attention layer is O(n^2 Â· d). Convolutional layers are generally more expensive than recurrent layers, by a factor of k, though separable convolutions decrease the complexity considerably. Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\",\n",
    "    \"We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\",\n",
    "    \"We varied the learning rate over the course of training, according to the formula: lrate = d^(-0.5)_model Â· min(step_num^(-0.5), step_num Â· warmup_steps^(-1.5)). This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.\",\n",
    "    \"We employ three types of regularization during training: Residual Dropout. We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_drop = 0.1. Label Smoothing. During training, we employed label smoothing of value Îµ_ls = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\",\n",
    "    \"On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\",\n",
    "    \"Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU. The Transformer (base model) EN-DE training cost is listed as 3.3 Â· 10^18 FLOPs, compared to GNMT + RL at 2.3 Â· 10^19 FLOPs, ConvS2S at 9.6 Â· 10^18 FLOPs, and ByteNet, Deep-Att + PosUnk, MoE and others at significantly higher costs.\",\n",
    "    \"In Table 3 rows (C) and (D) we observe that, as expected, bigger models are better. On the WMT 2014 English-to-German translation task, the big transformer model achieves a BLEU score of 28.4, compared to the base model's reported 27.3 in Table 2. For the big model, training took 3.5 days on 8 P100 GPUs, while the base model trained for 12 hours. The big model used d_model = 1024, d_ff = 4096, h = 16, and P_drop = 0.3, compared to the base model's smaller configuration.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2d45a93401afc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:07:56.825215Z",
     "start_time": "2025-11-30T10:06:22.652941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector database initialized at: D:\\Projects\\Multimodal-Retrieval-Augmented-Generation\\chroma_store\n",
      "âœ… Ollama is running.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ… Extracted 9 elements from PDF.\n",
      "ðŸ“„ Texts: 9, ðŸ“Š Tables: 0, ðŸ–¼ï¸ Images: 3\n",
      "âœ… Summarization complete.\n",
      "âœ… Added 9 text summaries.\n",
      "âš ï¸ No table summaries to add.\n",
      "âœ… Added 3 image summaries.\n",
      "âœ… Documents added to vector database.\n",
      "\n",
      "âœ… Retriever initialized and PDF ingested!\n",
      "ðŸ“Š Vectorstore collection: multi_modal_rag\n",
      "ðŸ“¦ Docstore contains: 12 original documents\n"
     ]
    }
   ],
   "source": [
    "ret = initialize_retriever()\n",
    "\n",
    "# Ingest your PDF (CRITICAL: This populates both vectorstore AND docstore)\n",
    "file = r\"D:\\Projects\\Multimodal-Retrieval-Augmented-Generation\\uploaded_pdfs\\NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
    "ingestion_chain(file, ret)\n",
    "\n",
    "print(\"\\nâœ… Retriever initialized and PDF ingested!\")\n",
    "print(f\"ðŸ“Š Vectorstore collection: {ret.vectorstore._collection.name}\")\n",
    "print(f\"ðŸ“¦ Docstore contains: {len(ret.docstore.store)} original documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90138cd78a56df9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:07:56.927563Z",
     "start_time": "2025-11-30T10:07:56.917749Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_ragas_dataset_v2(queries, gold_answers, retriever, output=\"ragas_testset_v2.csv\"):\n",
    "    \"\"\"\n",
    "    NEW VERSION: Retrieves ORIGINAL documents (not summaries)\n",
    "    Uses the MultiVectorRetriever that returns parent documents.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for q, ref in zip(queries, gold_answers):\n",
    "        print(f\"Processing: {q[:60]}...\")\n",
    "\n",
    "        # --- RETRIEVE ORIGINAL DOCUMENTS ---\n",
    "        # MultiVectorRetriever searches summaries but returns ORIGINAL content\n",
    "        docs = retrieve_documents(retriever=retriever, question=q, k=4)\n",
    "\n",
    "        # --- EXTRACT CONTEXT ---\n",
    "        # Parse docs returns {\"images\": [...], \"texts\": [...]}\n",
    "        context_obj = parse_docs(docs)\n",
    "\n",
    "        # Convert to strings for RAGAS\n",
    "        text_contexts = []\n",
    "        for doc in context_obj[\"texts\"]:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                text_contexts.append(doc.page_content)\n",
    "            elif hasattr(doc, 'text'):  # For unstructured elements\n",
    "                text_contexts.append(doc.text)\n",
    "            else:\n",
    "                text_contexts.append(str(doc))\n",
    "\n",
    "        # Images as base64 (RAGAS can't directly evaluate these, but we keep for completeness)\n",
    "        image_contexts = context_obj[\"images\"]\n",
    "\n",
    "        # Combine all contexts\n",
    "        retrieved_contexts = text_contexts + [f\"[IMAGE: base64 data of length {len(img)}]\" for img in image_contexts]\n",
    "\n",
    "        # --- GENERATE ANSWER ---\n",
    "        # This uses your full retrieval chain (including images in prompt)\n",
    "        answer = answer_question(q, retriever)\n",
    "\n",
    "        # --- ADD RAGAS ROW ---\n",
    "        rows.append({\n",
    "            \"user_input\": q,\n",
    "            \"retrieved_contexts\": retrieved_contexts,  # List of strings\n",
    "            \"response\": answer,\n",
    "            \"reference\": ref\n",
    "        })\n",
    "\n",
    "    # --- SAVE CSV ---\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output, index=False)\n",
    "    print(f\"\\nâœ… Saved RAGAS dataset â†’ {output}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386f7ffe04d919f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:09:06.336037Z",
     "start_time": "2025-11-30T10:07:56.957653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: How does the Transformer architecture model sequence depende...\n",
      "Processing: What are the main components of the encoder stack, and how d...\n",
      "Processing: How does the decoder use masking to preserve the autoregress...\n",
      "Processing: What mathematical steps define Scaled Dot-Product Attention ...\n",
      "Processing: What benefits does Multi-Head Attention provide compared to ...\n",
      "Processing: How do positional encodings represent token order, and why d...\n",
      "Processing: What role does the position-wise Feed-Forward Network play i...\n",
      "Processing: How does self-attention achieve a shorter maximum path lengt...\n",
      "Processing: What computational trade-offs does self-attention introduce ...\n",
      "Processing: What data, batching strategy, and vocabulary were used for t...\n",
      "Processing: How does the learning-rate schedule with warmup affect the t...\n",
      "Processing: What regularization methods (dropout, label smoothing) were ...\n",
      "Processing: What BLEU scores did the Transformer achieve on the WMT 2014...\n",
      "Processing: How does the Transformer's training cost compare to earlier ...\n",
      "Processing: What performance differences were observed when scaling from...\n",
      "\n",
      "âœ… Saved RAGAS dataset â†’ ragas_testset_v2.csv\n"
     ]
    }
   ],
   "source": [
    "testset_df = build_ragas_dataset_v2(queries, golden_answers, ret, output=\"ragas_testset_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5a90eff65f8b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:09:06.436471Z",
     "start_time": "2025-11-30T10:09:06.402559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Evaluation dataset ready with 15 samples\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"ragas_testset_v2.csv\")\n",
    "\n",
    "# Convert string representation of lists back to actual lists\n",
    "data[\"retrieved_contexts\"] = data[\"retrieved_contexts\"].apply(ast.literal_eval)\n",
    "\n",
    "# Create RAGAS EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(data)\n",
    "\n",
    "print(f\"\\nâœ… Evaluation dataset ready with {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac013179c878275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:18:54.608821Z",
     "start_time": "2025-11-30T10:09:06.449996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Running RAGAS evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   8%|â–Š         | 7/90 [01:00<08:56,  6.47s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  20%|â–ˆâ–ˆ        | 18/90 [02:05<05:42,  4.76s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 20/90 [03:03<11:19,  9.70s/it]Exception raised in Job[13]: TimeoutError()\n",
      "Exception raised in Job[7]: TimeoutError()\n",
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 29/90 [03:28<05:49,  5.73s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 31/90 [03:40<05:35,  5.68s/it]Exception raised in Job[19]: TimeoutError()\n",
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 41/90 [05:20<08:19, 10.19s/it]Exception raised in Job[31]: TimeoutError()\n",
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/90 [05:28<03:18,  4.85s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 53/90 [06:27<05:28,  8.87s/it]Exception raised in Job[37]: TimeoutError()\n",
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 60/90 [06:39<02:26,  4.89s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 68/90 [07:28<01:44,  4.76s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 69/90 [07:37<01:56,  5.56s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 74/90 [08:24<02:02,  7.63s/it]Exception raised in Job[57]: TimeoutError()\n",
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 79/90 [08:38<00:49,  4.50s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 80/90 [08:50<00:56,  5.66s/it]Exception raised in Job[67]: TimeoutError()\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [09:40<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "evaluator_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    LLMContextRecall(),\n",
    "    ContextPrecision(),\n",
    "    Faithfulness(),\n",
    "    FactualCorrectness(),\n",
    "    AnswerRelevancy(),\n",
    "    SemanticSimilarity(),\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ”„ Running RAGAS evaluation...\")\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b3df90daefd1b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:18:54.702392Z",
     "start_time": "2025-11-30T10:18:54.660351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved metrics â†’ ragas_results_v2.csv\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š RAGAS EVALUATION SUMMARY\n",
      "============================================================\n",
      "                                  mean       std       min       max\n",
      "context_recall                0.916667  0.191589  0.333333  1.000000\n",
      "context_precision             0.981481  0.036747  0.916667  1.000000\n",
      "faithfulness                  0.800608  0.189847  0.400000  1.000000\n",
      "factual_correctness(mode=f1)  0.490714  0.215637  0.200000  0.800000\n",
      "answer_relevancy              0.960015  0.035466  0.902491  1.000000\n",
      "semantic_similarity           0.934785  0.021089  0.904920  0.971087\n"
     ]
    }
   ],
   "source": [
    "results = result.to_pandas()\n",
    "\n",
    "# Save results\n",
    "results.to_csv(\"ragas_results_v2.csv\", index=False)\n",
    "print(\"âœ… Saved metrics â†’ ragas_results_v2.csv\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š RAGAS EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metric_cols = [\n",
    "    'context_recall',\n",
    "    'context_precision',\n",
    "    'faithfulness',\n",
    "    'factual_correctness(mode=f1)',\n",
    "    'answer_relevancy',\n",
    "    'semantic_similarity'\n",
    "]\n",
    "\n",
    "summary = results[metric_cols].describe().T[['mean', 'std', 'min', 'max']]\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790c4040791b89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
