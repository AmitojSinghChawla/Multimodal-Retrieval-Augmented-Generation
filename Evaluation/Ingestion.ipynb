{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:41:25.698409Z",
     "start_time": "2026-01-20T11:41:25.689413Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chunk_exporter.py\n",
    "import json\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d47af30e1d504a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:41:25.728779Z",
     "start_time": "2026-01-20T11:41:25.723711Z"
    }
   },
   "outputs": [],
   "source": [
    "def export_text_chunks(texts, source_pdf, output_file=\"chunks.jsonl\"):\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        for text in texts:\n",
    "            raw_text = text.text if hasattr(text, \"text\") else str(text)\n",
    "\n",
    "            record = {\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"modality\": \"text\",\n",
    "                \"source_pdf\": source_pdf,\n",
    "                \"page_number\": None,\n",
    "                \"raw_text\": raw_text,\n",
    "                \"image_b64\": None,\n",
    "                \"gold_questions\": []\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd61e4e1944dcbf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:41:25.741790Z",
     "start_time": "2026-01-20T11:41:25.736788Z"
    }
   },
   "outputs": [],
   "source": [
    "def export_image_chunks(images_b64, source_pdf, output_file=\"chunks.jsonl\"):\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        for img in images_b64:\n",
    "            record = {\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"modality\": \"image\",\n",
    "                \"source_pdf\": source_pdf,\n",
    "                \"page_number\": None,\n",
    "                \"raw_text\": None,\n",
    "                \"image_b64\": img,\n",
    "                \"gold_questions\": []\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e378179d7651769c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:41:50.779295Z",
     "start_time": "2026-01-20T11:41:25.757872Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Multimodal-Retrieval-Augmented-Generation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# App/Ingestion.py\n",
    "import os\n",
    "from App.Ingestion import create_chunks_from_pdf  # adjust import if needed\n",
    "\n",
    "\n",
    "def process_pdfs_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Iterate over all PDFs in a directory and yield (pdf_name, elements).\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            elements = create_chunks_from_pdf(file_path)\n",
    "            yield filename, elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29fc6c695008079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:42:24.183417Z",
     "start_time": "2026-01-20T11:41:50.786302Z"
    }
   },
   "outputs": [],
   "source": [
    "# App/Ingestion_chain.py\n",
    "from App.Ingestion import table_text_segregation, get_images\n",
    "from App.summarizer import summarize_texts_tables, summarize_images\n",
    "from App.VectorDB import add_documents_to_vector_db\n",
    "\n",
    "\n",
    "def ingestion_chain(pdf_directory, retriever):\n",
    "    \"\"\"\n",
    "    Complete ingestion pipeline:\n",
    "    Directory of PDFs ‚Üí extract ‚Üí export chunks ‚Üí summarize ‚Üí add to vector DB\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from App.ollama_running import ensure_ollama_running\n",
    "        ensure_ollama_running()\n",
    "\n",
    "        for pdf_name, elements in process_pdfs_in_directory(pdf_directory):\n",
    "            print(f\"\\nüìò Ingesting: {pdf_name}\")\n",
    "\n",
    "            # Segregate elements\n",
    "            tables, texts = table_text_segregation(elements)\n",
    "            images = get_images(elements)\n",
    "\n",
    "            print(\n",
    "                f\"üìÑ Texts: {len(texts)}, \"\n",
    "                f\"üìä Tables: {len(tables)}, \"\n",
    "                f\"üñºÔ∏è Images: {len(images)}\"\n",
    "            )\n",
    "\n",
    "            # ---- EXPORT RAW CHUNKS (PERSISTENT DATASET) ----\n",
    "            export_text_chunks(\n",
    "                texts=texts,\n",
    "                source_pdf=pdf_name\n",
    "            )\n",
    "\n",
    "            export_image_chunks(\n",
    "                images_b64=images,\n",
    "                source_pdf=pdf_name\n",
    "            )\n",
    "\n",
    "            print(\"‚úÖ Chunks exported to chunks.jsonl\")\n",
    "\n",
    "            # ---- DENSE SUMMARIZATION ----\n",
    "            text_summaries, table_summaries = summarize_texts_tables(texts, tables)\n",
    "            img_summaries = summarize_images(images)\n",
    "\n",
    "            print(\"‚úÖ Summarization complete\")\n",
    "\n",
    "            # ---- VECTOR DB INGESTION (RAG ONLY) ----\n",
    "            add_documents_to_vector_db(\n",
    "                texts,\n",
    "                text_summaries,\n",
    "                tables,\n",
    "                table_summaries,\n",
    "                images,\n",
    "                img_summaries,\n",
    "                retriever=retriever\n",
    "            )\n",
    "\n",
    "            print(\"‚úÖ Added to vector database\")\n",
    "\n",
    "        print(\"\\nüéØ Ingestion completed for all PDFs.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in ingestion pipeline: {str(e)}\")\n",
    "        raise RuntimeError(f\"Ingestion failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd903f8eebe4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from App.console_app import initialize_retriever\n",
    "retriever = initialize_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfcebf1606226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_chain(pdf_directory=r\"D:\\Projects\\Multimodal-Retrieval-Augmented-Generation\\Evaluation\", retriever=retriever)  # Replace with actual path and retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
